import os
import streamlit as st
import re
import difflib

# =============================================
# File: pages/1_í¬í† ë¦¬ì†Œê·¸ë˜í”¼.py
# =============================================

from langchain.chains import RetrievalQA


st.set_page_config(page_title="í¬í† ë¦¬ì†Œê·¸ë˜í”¼", layout="wide")

# ---------------- ìœ ì‚¬ë„ íŒì • ìœ í‹¸ ----------------
_STOPWORDS = {"the","a","an","of","and","to","in","port","on","for","with","by","at","from","is","are","was","were","be","as",
              "ë°","ê³¼","ì™€","ì—ì„œ","ìœ¼ë¡œ","ìœ¼ë¡œì¨","ì—","ì˜","ë¥¼","ì„","ì€","ëŠ”","ì´ë‹¤","í•œë‹¤","í•˜ëŠ”","ë˜ëŠ”"}

def _normalize_text(s: str) -> list[str]:
    s = (s or "").lower()
    s = re.sub(r"[^0-9a-zê°€-í£\s]", " ", s)
    toks = [t for t in s.split() if t and t not in _STOPWORDS]
    return toks

def _jaccard(a: list[str], b: list[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / len(sa | sb)

def is_similar(q: str, p: str, jaccard_thr: float = 0.55, ratio_thr: float = 0.70) -> bool:
    ta, tb = _normalize_text(q), _normalize_text(p)
    if _jaccard(ta, tb) >= jaccard_thr:
        return True
    if difflib.SequenceMatcher(None, " ".join(ta), " ".join(tb)).ratio() >= ratio_thr:
        return True
    return False

# ---------------- í˜ì´ì§€ ë³¸ë¬¸ ----------------
st.header("1) í¬í† ë¦¬ì†Œê·¸ë˜í”¼")

# ê°œìš”
st.subheader("ê°œìš”")
st.write("ì›¨ì´í¼ í‘œë©´ì— ê°ê´‘ë§‰ì„ ë°”ë¥´ê³  ë…¸ê´‘Â·í˜„ìƒìœ¼ë¡œ íŒ¨í„´ì„ í˜•ì„±í•©ë‹ˆë‹¤.")

# í•µì‹¬ í¬ì¸íŠ¸ (íˆ´íŒ)
st.subheader("í•µì‹¬ í¬ì¸íŠ¸")
st.markdown("""
- <span title="ê°ê´‘ë§‰ì„ ì›¨ì´í¼ì— ê· ì¼í•˜ê²Œ ë„í¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">PR ì½”íŒ…</span> â†’ 
  <span title="ê°ê´‘ë§‰ì˜ ìš©ë§¤ë¥¼ ì¦ë°œì‹œì¼œ ì•ˆì •í™”ì‹œí‚¤ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">ì†Œí”„íŠ¸ë² ì´í¬</span> â†’ 
  <span title="ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ë¹›(EUV/DUV)ì„ í†µí•´ ê°ê´‘ë§‰ì— ì „ì‚¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">ë…¸ê´‘(EUV/DUV)</span> â†’ 
  <span title="ë…¸ê´‘ í›„ Bakeë¥¼ í†µí•´ í™”í•™ ë°˜ì‘ì„ ì•ˆì •í™”ì‹œí‚¤ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">PEB</span> â†’ 
  <span title="ë…¸ê´‘ëœ ì˜ì—­ì„ í˜„ìƒì•¡ìœ¼ë¡œ ì œê±°í•˜ì—¬ íŒ¨í„´ì„ í˜•ì„±í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">í˜„ìƒ</span> â†’ 
  <span title="íŒ¨í„´ì„ ê³ ì •í•˜ê³  ë‚´ì—´ì„± ë° ë‚´í™”í•™ì„±ì„ ê°•í™”í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">í•˜ë“œë² ì´í¬</span> â†’ 
  <span title="íŒ¨í„´ì˜ ê²°í•¨ ì—¬ë¶€, ì •ë ¬ ìƒíƒœ ë“±ì„ ê²€ì‚¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">ê²€ì‚¬</span>
""", unsafe_allow_html=True)
st.markdown("- í•´ìƒë„(Î», NA, k1), í¬ì»¤ìŠ¤/ë„ì¦ˆ, LER/LWR")

st.subheader("í”„ë¡œì„¸ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨")
steps = ["Wafer Clean", "PR Coat", "Soft Bake", "Exposure", "PEB", "Develop", "Hard Bake", "Inspection"]
st.graphviz_chart("\n".join([
    "digraph G {",
    "rankdir=LR;",
    "node [shape=box, style=rounded, fontsize=12];",
    *[f"n{i} [label=\"{s}\"];" for i, s in enumerate(steps)],
    *[f"n{i} -> n{i + 1};" for i in range(len(steps) - 1)],
    "}",
]), use_container_width=True)

# ê³µì • ë‹¨ê³„ ì„¤ëª… + ì§„ë„ ê´€ë¦¬
st.subheader("ê³µì • ë‹¨ê³„ ì„¤ëª… ë° ì§„ë„ ê´€ë¦¬")

# ë‹¨ê³„ ë°ì´í„°
steps_data = [
    {
        "name": "ì›¨ì´í¼ ì„¸ì • (Wafer Clean)",
        "desc": """
ğŸ§¼ **ì„¤ëª…: ì›¨ì´í¼ ì„¸ì •(Wafer Clean)**

ì›¨ì´í¼ ì„¸ì •ì€ ë°˜ë„ì²´ ê³µì •ì˜ ì‹œì‘ì´ì í’ˆì§ˆì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ë‹¨ê³„ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ 'ì²­ì†Œ'ê°€ ì•„ë‹™ë‹ˆë‹¤.  
ì´ ë‹¨ê³„ì—ì„œ ì˜¤ì—¼ë¬¼ í•˜ë‚˜, íŒŒí‹°í´ í•˜ë‚˜ê°€ ìˆ˜ìœ¨ì„ ë–¨ì–´ëœ¨ë¦¬ê³  ë¶ˆëŸ‰ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì™œ ì„¸ì •ì´ ì¤‘ìš”í•œê°€?**
- ê°ê´‘ë§‰ ë„í¬ ì „, í‘œë©´ì´ ì™„ë²½íˆ ê¹¨ë—í•´ì•¼ íŒ¨í„´ì´ ì •í™•íˆ í˜•ì„±ë©ë‹ˆë‹¤.
- ì˜¤ì—¼ë¬¼ì€ PR ë„í¬ ë¶ˆê· ì¼, ë…¸ê´‘ ì˜¤ë¥˜, ì‹ê° ë¶ˆëŸ‰ì„ ìœ ë°œí•©ë‹ˆë‹¤.

**ì œê±° ëŒ€ìƒ**
- ìœ ê¸°ë¬¼ (PR ì”ë¥˜ë¬¼, ì‚¬ëŒ ì† ë“±)
- ê¸ˆì† ì´ì˜¨ (Cu, Fe ë“±)
- íŒŒí‹°í´ (ë¨¼ì§€, ë¯¸ì„¸ ì…ì)

**ì„¸ì • ë°©ì‹**
- RCA ì„¸ì •: í™”í•™ ìš©ì•¡ìœ¼ë¡œ ìœ ê¸°ë¬¼Â·ê¸ˆì† ì œê±°
- DIW ì„¸ì •: ì´ˆìˆœìˆ˜ë¡œ ë¬¼ë¦¬ì  ì„¸ì •
- í”Œë¼ì¦ˆë§ˆ ì„¸ì •: ì‚°ì†Œ í”Œë¼ì¦ˆë§ˆë¡œ ìœ ê¸°ë¬¼ ë¶„í•´

**ì„¸ì • í›„ í™•ì¸**
- AFM/SEMìœ¼ë¡œ í‘œë©´ ë¶„ì„
- ë“œë¼ì´ ê³µì •ìœ¼ë¡œ ìˆ˜ë¶„ ì œê±°
- í´ë¦°ë£¸ ê´€ë¦¬ë¡œ ì¬ì˜¤ì—¼ ë°©ì§€

ğŸ¯ í•µì‹¬ ìš”ì•½:
- ì›¨ì´í¼ ì„¸ì •ì€ ê³µì •ì˜ í’ˆì§ˆ ê¸°ì¤€ì„ 
- RCA ì„¸ì •ì€ ì‹œí—˜ ë‹¨ê³¨
- ì„¸ì • ì‹¤íŒ¨ëŠ” ê³µì • ì „ì²´ ì‹¤íŒ¨ë¡œ ì´ì–´ì§
""",
        "icon": "ğŸ§¼"
    },

    {
        "name": "ê°ê´‘ë§‰ ë„í¬ (PR Coat)",
        "desc": """
    ğŸ§´ **ê°•ì‚¬ ì„¤ëª…: ê°ê´‘ë§‰ ë„í¬ (Photoresist Coating)**

    ê°ê´‘ë§‰(PR)ì€ ë¹›ì— ë°˜ì‘í•˜ëŠ” íŠ¹ìˆ˜ í™”í•™ë¬¼ì§ˆë¡œ, ë…¸ê´‘ ì‹œ ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ì›¨ì´í¼ì— ì „ì‚¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•µì‹¬ ì¬ë£Œì…ë‹ˆë‹¤.  
    ì´ê±¸ ê· ì¼í•˜ê²Œ ë„í¬í•˜ì§€ ì•Šìœ¼ë©´ ì´í›„ ë…¸ê´‘, í˜„ìƒ, ì‹ê° ê³µì •ì—ì„œ **íŒ¨í„´ ì™œê³¡, í•´ìƒë„ ì €í•˜, ë¶ˆëŸ‰ ë°œìƒ**ì´ ì¼ì–´ë‚©ë‹ˆë‹¤.

    ğŸ” **ì™œ ë„í¬í•˜ëŠ”ê°€?**
    - PRì€ ë¹›ì— ë°˜ì‘í•˜ì—¬ íŒ¨í„´ì„ í˜•ì„±í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì¸µ
    - ë…¸ê´‘ ì‹œ ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ì •í™•íˆ ì „ì‚¬í•˜ê¸° ìœ„í•œ ì¤€ë¹„ ë‹¨ê³„

    ğŸŒ€ **ë„í¬ ë°©ì‹: Spin Coating**
    - ì›¨ì´í¼ë¥¼ ê³ ì† íšŒì „ì‹œì¼œ PRì„ ê· ì¼í•˜ê²Œ í¼ëœ¨ë¦¬ëŠ” ë°©ì‹
    - PRì˜ **ì ë„**, **íšŒì „ ì†ë„(rpm)**, **ë„í¬ ì‹œê°„**ì´ ë‘ê»˜ì™€ ê· ì¼ì„±ì— ì˜í–¥

    ğŸ”¥ **ë„í¬ í›„ ë°”ë¡œ Soft Bake**
    - PR ë‚´ ìš©ë§¤ë¥¼ ì¦ë°œì‹œì¼œ ì•ˆì •í™”
    - Bakeë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ë…¸ê´‘ ì‹œ íŒ¨í„´ ë²ˆì§ ë°œìƒ

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ì¤‘ì‹¬ë¶€ì™€ ê°€ì¥ìë¦¬ ë‘ê»˜ ì°¨ì´ â†’ ê· ì¼ì„± í™•ë³´ ì¤‘ìš”
    - PR ì˜¤ì—¼ ë°©ì§€ â†’ í´ë¦°ë£¸ ê´€ë¦¬ ì² ì €
    - PR ì¢…ë¥˜: Positive vs Negative â†’ ë…¸ê´‘ í›„ ì œê±° ë°©ì‹ ë‹¤ë¦„

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - ê°ê´‘ë§‰ì€ íŒ¨í„´ í˜•ì„±ì˜ ê¸°ë°˜ì¸µ
    - Spin Coatingì€ ì†ë„Â·ì ë„Â·ì‹œê°„ì´ í•µì‹¬
    - ë„í¬ í›„ Soft BakeëŠ” í•„ìˆ˜
    - PR ê· ì¼ì„±ì€ í•´ìƒë„ í’ˆì§ˆì„ ì¢Œìš°
    """,
        "icon": "ğŸ§´"
    },

    {
        "name": "ì†Œí”„íŠ¸ ë² ì´í¬ (Soft Bake)",
        "desc": """
    ğŸ”¥ **ê°•ì‚¬ ì„¤ëª…: ì†Œí”„íŠ¸ ë² ì´í¬ (Soft Bake)**

    ê°ê´‘ë§‰(PR)ì„ ë„í¬í•œ ì§í›„ì—ëŠ” ë‚´ë¶€ì— ìš©ë§¤ê°€ ë‚¨ì•„ ìˆì–´, ë…¸ê´‘ ì‹œ ë¹›ì´ ë²ˆì§€ê³  íŒ¨í„´ í’ˆì§ˆì´ ì €í•˜ë©ë‹ˆë‹¤.  
    ì†Œí”„íŠ¸ ë² ì´í¬ëŠ” ì´ ìš©ë§¤ë¥¼ ì¦ë°œì‹œì¼œ PRì„ ì•ˆì •í™”ì‹œí‚¤ëŠ” í•µì‹¬ ë‹¨ê³„ì…ë‹ˆë‹¤.

    ğŸ§ª **Bake ì¡°ê±´**
    - ì˜¨ë„: 90~110Â°C (ë„ˆë¬´ ë†’ìœ¼ë©´ PR ê²½í™”)
    - ì‹œê°„: ìˆ˜ì‹­ ì´ˆ~ìˆ˜ ë¶„ (PR ì¢…ë¥˜ì— ë”°ë¼ ì¡°ì ˆ)
    - ì¥ë¹„: Hot Plate ë˜ëŠ” Oven (Hot Plateê°€ ë” ê· ì¼)

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ê³¼ë„í•œ Bake â†’ PR ê²½í™”ë¡œ ë…¸ê´‘ ë°˜ì‘ ì €í•˜
    - ë¶ˆì¶©ë¶„í•œ Bake â†’ ìš©ë§¤ ì”ë¥˜ë¡œ íŒ¨í„´ íë¦¼
    - ê¸‰ì† ëƒ‰ê° â†’ ê· ì—´ ë°œìƒ ê°€ëŠ¥

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - ì†Œí”„íŠ¸ ë² ì´í¬ëŠ” PR ì•ˆì •í™”ì˜ í•µì‹¬
    - Bake ì¡°ê±´ì´ íŒ¨í„´ í’ˆì§ˆì„ ì¢Œìš°
    - ì‹¤íŒ¨ ì‹œ ë…¸ê´‘ í’ˆì§ˆ ì €í•˜
    """,
        "icon": "ğŸ”¥"
    },
    {
        "name": "ë…¸ê´‘ (Exposure)",
        "desc": """
    ğŸ’¡ **ê°•ì‚¬ ì„¤ëª…: ë…¸ê´‘ (Exposure)**

    ë…¸ê´‘ì€ ê°ê´‘ë§‰(PR)ì— ë¹›ì„ ì¡°ì‚¬í•˜ì—¬ ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ì „ì‚¬í•˜ëŠ” ê³µì •ì…ë‹ˆë‹¤.  
    ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë¹›ì˜ ì¢…ë¥˜ì™€ ì¡°ì‚¬ ì¡°ê±´ì´ íŒ¨í„´ì˜ ì •ë°€ë„ì™€ í•´ìƒë„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

    ğŸ”¬ **í•´ìƒë„ ê³µì‹**
    í•´ìƒë„ â‰ˆ kâ‚ Ã— (Î» / NA)
    - Î»: íŒŒì¥ (ì§§ì„ìˆ˜ë¡ ì¢‹ìŒ) â†’ EUV(13.5nm) > DUV(193nm)
    - NA: ê°œêµ¬ìˆ˜ (í´ìˆ˜ë¡ ì¢‹ìŒ)
    - kâ‚: ê³µì • ê³„ìˆ˜ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)

    ğŸ§ª **ë…¸ê´‘ ì¡°ê±´**
    - í¬ì»¤ìŠ¤(Focus): ì´ˆì  ì •í™•ë„
    - ë„ì¦ˆ(Dose): ë¹›ì˜ ì—ë„ˆì§€ëŸ‰
    - LER/LWR: íŒ¨í„´ì˜ ì„ (edge) ê±°ì¹ ê¸°

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ë§ˆìŠ¤í¬ ì •ë ¬ ì˜¤ì°¨ â†’ íŒ¨í„´ ìœ„ì¹˜ ì˜¤ë¥˜
    - ì§„ë™, ì˜¨ë„ ë³€í™” â†’ í¬ì»¤ìŠ¤ ë¶ˆì•ˆì •
    - PR ì¢…ë¥˜ì— ë”°ë¼ ë°˜ì‘ ë¯¼ê°ë„ ë‹¤ë¦„

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - ë…¸ê´‘ì€ íŒ¨í„´ ì •ë°€ë„ì™€ í•´ìƒë„ë¥¼ ê²°ì •í•˜ëŠ” í•µì‹¬ ê³µì •
    - EUVëŠ” ë” ë¯¸ì„¸í•œ íŒ¨í„´ êµ¬í˜„ ê°€ëŠ¥
    - í¬ì»¤ìŠ¤Â·ë„ì¦ˆÂ·LER/LWRì´ íŒ¨í„´ í’ˆì§ˆì„ ì¢Œìš°
    """,
        "icon": "ğŸ’¡"
    },
    {
        "name": "PEB (Post-Exposure Bake)",
        "desc": """
    â™¨ï¸ **ì„¤ëª…: PEB (Post-Exposure Bake)**

    PEBëŠ” ë…¸ê´‘ í›„ ê°ê´‘ë§‰(PR)ì— ë°œìƒí•œ í™”í•™ ë°˜ì‘ì„ ì•ˆì •í™”ì‹œí‚¤ëŠ” ì—´ì²˜ë¦¬ ê³µì •ì…ë‹ˆë‹¤.  
    íŠ¹íˆ Chemically Amplified Resist(CAR)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì‚° ìƒì„±ê³¼ í™•ì‚°ì´ íŒ¨í„´ í’ˆì§ˆì„ ì¢Œìš°í•©ë‹ˆë‹¤.

    ğŸ”¬ **PEBì˜ ì—­í• **
    - ì‚° í™•ì‚° ì œì–´ â†’ PR ë°˜ì‘ ìœ ë„
    - LER ê°œì„  â†’ íŒ¨í„´ ê²½ê³„ ì„ ëª…ë„ í–¥ìƒ
    - í•´ìƒë„ í–¥ìƒ â†’ Bake ì¡°ê±´ì— ë”°ë¼ ê²°ì •

    ğŸ§ª **Bake ì¡°ê±´**
    - ì˜¨ë„: 90~130Â°C
    - ì‹œê°„: ìˆ˜ì‹­ ì´ˆ~ìˆ˜ ë¶„
    - ì¥ë¹„: Hot Plate ì‚¬ìš©

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ê³¼ë„í•œ Bake â†’ ì‚° í™•ì‚° ê³¼ë‹¤ë¡œ íŒ¨í„´ ë²ˆì§
    - ë¶€ì¡±í•œ Bake â†’ ë°˜ì‘ ë¶ˆì™„ì „
    - ê¸‰ì† ëƒ‰ê° â†’ PR ê· ì—´ ê°€ëŠ¥

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - PEBëŠ” ë…¸ê´‘ í›„ í™”í•™ ë°˜ì‘ ì•ˆì •í™” ê³µì •
    - ì‚° í™•ì‚°ì´ íŒ¨í„´ í’ˆì§ˆì„ ì¢Œìš°
    - Bake ì¡°ê±´ì´ í•´ìƒë„ì™€ ìˆ˜ìœ¨ì— ì˜í–¥
    """,
        "icon": "â™¨ï¸"
    },
    {
        "name": "í˜„ìƒ (Develop)",
        "desc": """
    ğŸ§ª **ì¼íƒ€ê°•ì‚¬ ì„¤ëª…: í˜„ìƒ (Develop)**

    í˜„ìƒì€ ë…¸ê´‘ëœ ê°ê´‘ë§‰(PR) ì˜ì—­ì„ í˜„ìƒì•¡ìœ¼ë¡œ ì œê±°í•˜ì—¬ ì‹¤ì œ íŒ¨í„´ì„ í˜•ì„±í•˜ëŠ” ê³µì •ì…ë‹ˆë‹¤.  
    ì´ ë‹¨ê³„ì—ì„œ íŒ¨í„´ì´ ëˆˆì— ë³´ì´ëŠ” êµ¬ì¡°ë¡œ ì™„ì„±ë˜ë©°, PR ì¢…ë¥˜ì— ë”°ë¼ ì œê±°ë˜ëŠ” ì˜ì—­ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

    ğŸ” **Positive vs Negative PR**
    - Positive PR: ë…¸ê´‘ëœ ì˜ì—­ì´ ì œê±°ë¨
    - Negative PR: ë…¸ê´‘ëœ ì˜ì—­ì´ ë‚¨ìŒ

    ğŸ§ª **í˜„ìƒ ì¡°ê±´**
    - í˜„ìƒì•¡ ë†ë„: ê³¼í˜„ìƒ/ë¯¸í˜„ìƒ ë°©ì§€
    - ì‹œê°„: PR ë‘ê»˜ì— ë”°ë¼ ì¡°ì ˆ
    - ì˜¨ë„: ì¼ì • ìœ ì§€ë¡œ ë°˜ì‘ ì•ˆì •í™”

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ê³¼í˜„ìƒ â†’ íŒ¨í„´ ì†ìƒ
    - ë¯¸í˜„ìƒ â†’ íŒ¨í„´ ë¯¸í˜•ì„±
    - í˜„ìƒ í›„ ì„¸ì • â†’ ì”ë¥˜ë¬¼ ì œê±° í•„ìˆ˜

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - í˜„ìƒì€ íŒ¨í„´ì„ ì‹¤ì œë¡œ í˜•ì„±í•˜ëŠ” í•µì‹¬ ê³µì •
    - PR ì¢…ë¥˜ì— ë”°ë¼ ì œê±° ë°©ì‹ ë‹¬ë¼ì§
    - ì¡°ê±´ì´ íŒ¨í„´ í’ˆì§ˆì„ ì¢Œìš°
    """,
        "icon": "ğŸ§ª"
    },

    {
        "name": "í•˜ë“œ ë² ì´í¬ (Hard Bake)",
        "desc": """
    ğŸ§± **ì¼íƒ€ê°•ì‚¬ ì„¤ëª…: í•˜ë“œ ë² ì´í¬ (Hard Bake)**

    í•˜ë“œ ë² ì´í¬ëŠ” í˜„ìƒ í›„ í˜•ì„±ëœ íŒ¨í„´ì„ ê³ ì˜¨ì—ì„œ ì—´ì²˜ë¦¬í•˜ì—¬ êµ¬ì¡°ë¥¼ ê³ ì •í•˜ê³ , ì´í›„ ê³µì •ì—ì„œ ë¬¼ë¦¬Â·í™”í•™ì  ì†ìƒì— ê²¬ë”œ ìˆ˜ ìˆë„ë¡ ê°•í™”í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

    ğŸ”¬ **í•˜ë“œ ë² ì´í¬ì˜ ëª©ì **
    - íŒ¨í„´ ê³ ì • â†’ PR ê²½í™”ë¡œ êµ¬ì¡° ì•ˆì •í™”
    - ë‚´ì—´ì„± í–¥ìƒ â†’ ê³ ì˜¨ ê³µì •ì—ì„œë„ íŒ¨í„´ ìœ ì§€
    - ë‚´í™”í•™ì„± ê°•í™” â†’ ì‹ê°ì•¡, í”Œë¼ì¦ˆë§ˆ ë“± ê³µê²©ì— ëŒ€í•œ ì €í•­ì„± ì¦ê°€

    ğŸ§ª **Bake ì¡°ê±´**
    - ì˜¨ë„: 120~150Â°C
    - ì‹œê°„: ìˆ˜ ë¶„ ì´ìƒ
    - ì¥ë¹„: Hot Plate ë˜ëŠ” Oven

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ê³¼ë„í•œ Bake â†’ PR ê°ˆë³€, íŒ¨í„´ ì†ìƒ
    - ë¶€ì¡±í•œ Bake â†’ ë‚´ì—´ì„±Â·ë‚´í™”í•™ì„± ë¶€ì¡±
    - ê¸‰ì† ëƒ‰ê° â†’ ê· ì—´ ë°œìƒ ê°€ëŠ¥

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - í•˜ë“œ ë² ì´í¬ëŠ” íŒ¨í„´ì„ ê³ ì •í•˜ê³  ë³´í˜¸í•˜ëŠ” ì—´ì²˜ë¦¬ ê³µì •
    - Bake ì¡°ê±´ì´ íŒ¨í„´ ì•ˆì •ì„±ê³¼ ë‚´êµ¬ì„±ì„ ê²°ì •
    - ì´í›„ ê³µì •ì—ì„œ íŒ¨í„´ ì†ìƒì„ ë°©ì§€í•˜ëŠ” ë°©íŒ¨ ì—­í• 
    """,
        "icon": "ğŸ§±"
    },

    {
        "name": "ê²€ì‚¬ (Inspection)",
        "desc": """
    ğŸ” **ì¼íƒ€ê°•ì‚¬ ì„¤ëª…: ê²€ì‚¬ (Inspection)**

    ê²€ì‚¬ëŠ” í¬í† ë¦¬ì†Œê·¸ë˜í”¼ ê³µì •ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, í˜•ì„±ëœ íŒ¨í„´ì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê³  ë‹¤ìŒ ê³µì •ìœ¼ë¡œ ë„˜ê¸¸ì§€ë¥¼ ê²°ì •í•˜ëŠ” í’ˆì§ˆ ê²Œì´íŠ¸ì…ë‹ˆë‹¤.

    ğŸ§ª **ê²€ì‚¬ í•­ëª©**
    - ì •ë ¬ ì •í™•ë„: ë§ˆìŠ¤í¬ íŒ¨í„´ ìœ„ì¹˜ í™•ì¸
    - ê²°í•¨ ê²€ì‚¬: ëˆ„ë½, ê³¼í˜„ìƒ, ì”ë¥˜ë¬¼ ë“±
    - ì¹˜ìˆ˜ ì¸¡ì •(CD): íŒ¨í„´ ì„ í­ì´ ì„¤ê³„ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€
    - í‘œë©´ ê±°ì¹ ê¸°(LER): ì—£ì§€ í’ˆì§ˆ í‰ê°€

    ğŸ”¬ **ê²€ì‚¬ ì¥ë¹„**
    - SEM, CD-SEM, Overlay Tool, AOI ë“±

    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - ê²€ì‚¬ ëˆ„ë½ â†’ ë¶ˆëŸ‰ ì›¨ì´í¼ í†µê³¼
    - ê³¼ë„í•œ ê²€ì‚¬ â†’ ì‹œê°„Â·ë¹„ìš© ì¦ê°€
    - ê¸°ì¤€ ë¯¸ì„¤ì • â†’ ë¶ˆëŸ‰ íŒë‹¨ ëª¨í˜¸

    ğŸ¯ **í•µì‹¬ ìš”ì•½**
    - ê²€ì‚¬ëŠ” íŒ¨í„´ í’ˆì§ˆì„ ìµœì¢… í™•ì¸í•˜ëŠ” í’ˆì§ˆ ê²Œì´íŠ¸
    - ê²€ì‚¬ ê²°ê³¼ëŠ” ê³µì • í”¼ë“œë°±ìœ¼ë¡œë„ í™œìš©ë¨
    - ì •ë ¬Â·ê²°í•¨Â·ì¹˜ìˆ˜Â·ê±°ì¹ ê¸° ë“± ë‹¤ê°ë„ í‰ê°€ í•„ìˆ˜
    """,
        "icon": "ğŸ”"
    }

]


# ì§„ë„ ìƒíƒœ ì´ˆê¸°í™”
if "progress" not in st.session_state:
    st.session_state.progress = {step["name"]: False for step in steps_data}

# ë‹¨ê³„ë³„ ì„¤ëª… ë° ì²´í¬ë°•ìŠ¤
completed = 0
for step in steps_data:
    with st.expander(f"{step['icon']} {step['name']}"):
        st.write(step["desc"])
        checked = st.checkbox("ì´ ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ", value=st.session_state.progress[step["name"]], key=step["name"])
        st.session_state.progress[step["name"]] = checked
        if checked:
            completed += 1

# ì „ì²´ ì§„ë„ìœ¨ í‘œì‹œ
total = len(steps_data)
percent = int((completed / total) * 100)
st.progress(percent)
st.caption(f"ğŸ“˜ í•™ìŠµ ì§„ë„: {completed} / {total} ë‹¨ê³„ ì™„ë£Œ ({percent}%)")

# ---------------- ì§ˆì˜ì‘ë‹µ (RAG) ----------------
st.subheader("ì§ˆì˜ì‘ë‹µ (RAG)")
if "vectorstore" not in st.session_state:
    st.info("ì„ë² ë”© ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ì—ì„œ PDF ì—…ë¡œë“œ â†’ ì„ë² ë”© ìƒì„± í›„ ì´ìš©í•˜ì„¸ìš”.")
else:
    if "qa_chain" not in st.session_state:
        backend = st.session_state.get("llm_backend", "openai")
        model = st.session_state.get("llm_model", "gpt-4o-mini")
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 4})
        if backend == "openai":
            try:
                from langchain_openai import ChatOpenAI
                llm = ChatOpenAI(model=model, temperature=0.2)
            except Exception:
                # í´ë°±: OpenAI SDK (ì„ íƒ)
                from openai import OpenAI
                _client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY",""))
                class _OpenAILLM:
                    def invoke(self, prompt):
                        r = _client.chat.completions.create(model=model, messages=[{"role":"user","content":prompt}], temperature=0.2)
                        class _R: content = r.choices[0].message.content
                        return _R()
                llm = _OpenAILLM()
        else:
            from langchain_community.chat_models import ChatOllama
            llm = ChatOllama(model=model, temperature=0.2)

        st.session_state.qa_chain = RetrievalQA.from_chain_type(
            llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True
        )

    q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: EUVì™€ DUV ì°¨ì´")
    if st.button("ì§ˆë¬¸í•˜ê¸°", use_container_width=True):
        if q.strip():
            out = st.session_state.qa_chain({"query": q})
            st.markdown("### ë‹µë³€")
            st.write(out.get("result", "ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"))
            src = out.get("source_documents") or []
            if src:
                with st.expander("ì¶œì²˜"):
                    for i, sdoc in enumerate(src, 1):
                        meta = sdoc.metadata or {}
                        st.write(f"{i}. {meta.get('source', 'íŒŒì¼')} p.{meta.get('page', '?')}")
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ---------------- ëœë¤ ë¬¸ì œ ìƒì„±ê¸° + ì±„ì  ----------------
st.subheader("ëœë¤ ë¬¸ì œ ìƒì„±ê¸°")
CATEGORY_NAME = "í¬í† ë¦¬ì†Œê·¸ë˜í”¼"  # â† í˜ì´ì§€ ì£¼ì œëª…

# (ì¤‘ë³µ íšŒí”¼ìš© íˆìŠ¤í† ë¦¬)
hist_key = f"{CATEGORY_NAME}_quiz_history"
if hist_key not in st.session_state:
    st.session_state[hist_key] = []  # ë¬¸ìì—´(ì„œìˆ í˜• ì§ˆë¬¸) ë˜ëŠ” MC ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì €ì¥

# ì„¤ì •
cols = st.columns(3)
# âœ… ë‚œì´ë„ ë‘ ê°€ì§€ë§Œ
difficulty   = cols[0].selectbox(
    "ë‚œì´ë„",
    ["ì´ˆê¸‰", "ê³ ê¸‰"],
    index=0,
    key=f"{CATEGORY_NAME}_difficulty"      # â† ê³ ìœ  key ì¶”ê°€
)
n_items      = cols[1].selectbox(
    "ë¬¸í•­ ìˆ˜",
    [1, 3, 5],
    index=1,
    key=f"{CATEGORY_NAME}_n_items"         # â† ê³ ìœ  key ì¶”ê°€
)
has_vs       = "vectorstore" in st.session_state
with_context = cols[2].checkbox(
    "ì—…ë¡œë“œ ë¬¸ì„œ ê¸°ë°˜(ê¶Œì¥)",
    has_vs,
    key=f"{CATEGORY_NAME}_with_context"    # â† ê³ ìœ  key ì¶”ê°€(ê¶Œì¥)
)

# LLM í—¬í¼ (ê¸°ì¡´ê³¼ ë™ì¼)
def _get_llm_backend():
    return st.session_state.get("llm_backend", "openai"), st.session_state.get("llm_model", "gpt-4o")

def _generate_with_openai(prompt: str, model_name: str) -> str:
    try:
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model=model_name, temperature=0)
        return llm.invoke(prompt).content
    except Exception:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", ""))
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
            )
            return resp.choices[0].message.content
        except Exception as e:
            return f"__GEN_ERROR__ {e}"

def _generate_with_gemini(prompt: str, model_name: str) -> str:
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
        return llm.invoke(prompt).content
    except Exception:
        try:
            import google.generativeai as genai
            genai.configure(api_key=os.environ.get("GOOGLE_API_KEY", ""))
            model = genai.GenerativeModel(model_name)
            resp = model.generate_content(prompt)
            return resp.text
        except Exception as e:
            return f"__GEN_ERROR__ {e}"

def _gather_context(k: int = 6) -> str:
    if not has_vs or not with_context:
        return ""
    try:
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": k})
        docs = retriever.get_relevant_documents(f"{CATEGORY_NAME} í•µì‹¬ ê°œë… ìš”ì•½")
        return "\n\n".join(d.page_content for d in docs)[:6000]
    except Exception:
        return ""

# ----- ê³µí†µ: ì„œìˆ í˜• ì¶”ì¶œê¸° (ê³ ê¸‰ìš©)
def _extract_questions(s: str, expected_n: int) -> list[str]:
    s = (s or "").strip()
    if not s:
        return []
    parts = re.split(r'^\s*\d+[\.\)\]]\s+', s, flags=re.M)
    parts = [p.strip() for p in parts if p.strip()]
    if len(parts) <= 1:
        parts = [p.strip() for p in re.split(r'\n\s*\n+', s) if p.strip()]
    if len(parts) <= 1:
        lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
        parts = [(" ".join(lines))] if lines else []
    return parts[:expected_n]

# ----- ì´ˆê¸‰(MC) ì „ìš©: ìƒì„± í…œí”Œë¦¿ & íŒŒì„œ
QUIZ_PROMPT_MC = """\
ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê³µì • ê³¼ëª©ì˜ êµìˆ˜ì…ë‹ˆë‹¤.
ì£¼ì œ: {category}
ë‚œì´ë„: ì´ˆê¸‰
ì¶œì œ ë¬¸í•­ ìˆ˜: {n_items}

{context}

ìš”êµ¬ì‚¬í•­:
- 4ì§€ì„ ë‹¤ ê°ê´€ì‹ ë¬¸ì œë¥¼ {n_items}ê°œ ìƒì„±
- ê° ë¬¸í•­ì€ ë°˜ë“œì‹œ ì•„ë˜ 'ì •í™•í•œ í˜•ì‹'ì„ ì§€í‚¬ ê²ƒ (ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€)
- ë³´ê¸°ëŠ” A) B) C) D) ë¡œ í‘œì‹œ, ì •ë‹µì€ í•˜ë‚˜ë§Œ
- ê° ë¬¸í•­ì— ê°„ë‹¨í•œ í•´ì„¤ 1~2ë¬¸ì¥ í¬í•¨

[ì •í™•í•œ í˜•ì‹ ì˜ˆì‹œ â€” ì´ í‹€ì„ ê·¸ëŒ€ë¡œ ì§€í‚¬ ê²ƒ]
1) ì§ˆë¬¸ í…ìŠ¤íŠ¸
A) ë³´ê¸° A
B) ë³´ê¸° B
C) ë³´ê¸° C
D) ë³´ê¸° D
ì •ë‹µ: A
í•´ì„¤: í•œë‘ ë¬¸ì¥ ì„¤ëª…

"""

def _parse_mc_questions(s: str, expected_n: int):
    """LLM ì¶œë ¥ì—ì„œ ê°ê´€ì‹ ë¬¸í•­ì„ êµ¬ì¡°í™”í•˜ì—¬ [ {q:str, opts:list[A..D], answer:'A'..'D', expl:str}, ... ] ë¡œ ë°˜í™˜"""
    blocks = re.split(r'^\s*\d+\)\s+', s.strip(), flags=re.M)
    items = []
    for blk in blocks:
        blk = blk.strip()
        if not blk:
            continue
        lines = [ln.strip() for ln in blk.splitlines() if ln.strip()]
        # ì§ˆë¬¸: ë³´ê¸° ì‹œì‘ ì „ ì¤„ë“¤ í•©ì¹˜ê¸°
        q_lines = []
        opts = {'A': None, 'B': None, 'C': None, 'D': None}
        ans = None
        expl = ""
        phase = "q"
        for ln in lines:
            m_opt = re.match(r'^([ABCD])[)\.]\s*(.+)$', ln, flags=re.I)
            if m_opt:
                phase = "opt"
                key = m_opt.group(1).upper()
                opts[key] = m_opt.group(2).strip()
                continue
            m_ans = re.match(r'^ì •ë‹µ\s*:\s*([ABCD])\s*$', ln, flags=re.I)
            if m_ans:
                ans = m_ans.group(1).upper()
                phase = "ans"
                continue
            m_ex = re.match(r'^í•´ì„¤\s*:\s*(.*)$', ln, flags=re.I)
            if m_ex:
                expl = m_ex.group(1).strip()
                phase = "expl"
                continue
            if phase == "q":
                q_lines.append(ln)
            elif phase == "expl":
                expl += (" " + ln)
            # ë‚˜ë¨¸ì§€ëŠ” ë¬´ì‹œ
        qtext = " ".join(q_lines).strip()
        if qtext and all(opts[k] for k in ['A','B','C','D']) and ans in 'ABCD':
            items.append({
                "q": qtext,
                "opts": [f"A) {opts['A']}", f"B) {opts['B']}", f"C) {opts['C']}", f"D) {opts['D']}"],
                "answer": ans,
                "expl": expl.strip()
            })
        if len(items) >= expected_n:
            break
    return items

# ----- ê³ ê¸‰(ì„œìˆ í˜•) í…œí”Œë¦¿
QUIZ_PROMPT_TXT = """\
ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê³µì • ê³¼ëª©ì˜ êµìˆ˜ì…ë‹ˆë‹¤.
ì£¼ì œ: {category}
ë‚œì´ë„: ê³ ê¸‰
ì¶œì œ ë¬¸í•­ ìˆ˜: {n_items}

{context}

ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì£¼ì œì— ë§ëŠ” ëœë¤ ì„œìˆ í˜• ë¬¸ì œë¥¼ {n_items}ê°œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ë¬¸í•­ì€ 1), 2), 3)... ì²˜ëŸ¼ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ í•œ ì¤„ì”© ì‹œì‘í•˜ì„¸ìš”.
ë‹µì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

# ----- ì±„ì  í…œí”Œë¦¿(ì„œìˆ í˜•ìš©, ê¸°ì¡´ ìœ ì§€)
EVAL_PROMPT_TMPL = """\
ë‹¹ì‹ ì€ {category} ë¶„ì•¼ì˜ ì±„ì  ë³´ì¡°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸í•­ê³¼ ìˆ˜í—˜ì ë‹µì•ˆì„ í‰ê°€í•˜ì„¸ìš”.

[ë¬¸í•­]
{question}

[ìˆ˜í—˜ì ë‹µì•ˆ]
{answer}

(ì„ íƒ) ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{context}

í‰ê°€ ê¸°ì¤€:
- ì‚¬ì‹¤ ì¼ì¹˜ ì—¬ë¶€, í•µì‹¬ ê°œë… í¬í•¨ ì—¬ë¶€, ë…¼ë¦¬ì„±.
- ê°„ê²°íˆ 'ì •ë‹µ' ë˜ëŠ” 'ì˜¤ë‹µ'ìœ¼ë¡œ íŒì •í•˜ê³ , 2~3ë¬¸ì¥ì˜ í”¼ë“œë°± ì œê³µ.

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ì§€í‚¤ì„¸ìš”(ì¤„ë°”ê¿ˆ í¬í•¨, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€):
íŒì •: ì •ë‹µ|ì˜¤ë‹µ
í”¼ë“œë°±: <ë‘ì„¸ ë¬¸ì¥ í”¼ë“œë°±>
"""

# ----- ë¬¸ì œ ìƒì„± -----
if st.button("ëœë¤ ë¬¸ì œ ìƒì„±", use_container_width=True):
    # ì§„í–‰í‘œì‹œê°€ ì‚¬ë¼ì§€ë„ë¡ í”Œë ˆì´ìŠ¤í™€ë”
    ph = st.empty()
    with ph.container():
        if hasattr(st, "status"):
            with st.status("ë¬¸ì œ ìƒì„± ì¤‘...", expanded=True) as status:
                status.update(label="ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘...", state="running")
                backend, model = _get_llm_backend()
                context = _gather_context()

                status.update(label="í”„ë¡¬í”„íŠ¸ êµ¬ì„±...", state="running")
                if difficulty == "ì´ˆê¸‰":
                    prompt = QUIZ_PROMPT_MC.format(
                        category=CATEGORY_NAME,
                        n_items=n_items,
                        context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
                    )
                else:
                    prompt = QUIZ_PROMPT_TXT.format(
                        category=CATEGORY_NAME,
                        n_items=n_items,
                        context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
                    )

                status.update(label="ë¬¸í•­ ìƒì„± ìš”ì²­...", state="running")
                raw = _generate_with_openai(prompt, model) if backend == "openai" else _generate_with_gemini(prompt, model)

                prev_texts = [p if isinstance(p, str) else p.get("q","") for p in st.session_state[hist_key]]

                if difficulty == "ì´ˆê¸‰":
                    cand = _parse_mc_questions(raw, n_items)
                    uniques = []
                    for item in cand:
                        if not any(is_similar(item["q"], pt) for pt in prev_texts):
                            uniques.append(item)
                    # ë¶€ì¡± ì‹œ ë³´ê°• 1íšŒ
                    if len(uniques) < n_items:
                        need = n_items - len(uniques)
                        status.update(label=f"ë³´ê°• ìƒì„± ({need}ê°œ)...", state="running")
                        raw2 = _generate_with_openai(prompt, model) if backend == "openai" else _generate_with_gemini(prompt, model)
                        cand2 = _parse_mc_questions(raw2, need)
                        for it in cand2:
                            if len(uniques) >= n_items: break
                            if not any(is_similar(it["q"], pt) for pt in (prev_texts + [u["q"] for u in uniques])):
                                uniques.append(it)

                    st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                    st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ì´ˆê¸‰"
                    st.session_state[hist_key].extend([u["q"] for u in uniques])

                else:  # ê³ ê¸‰(ì„œìˆ í˜•)
                    cand = _extract_questions(raw, n_items)
                    uniques = []
                    for q in cand:
                        if not any(is_similar(q, pt) for pt in prev_texts):
                            uniques.append(q)
                    if len(uniques) < n_items:
                        need = n_items - len(uniques)
                        status.update(label=f"ë³´ê°• ìƒì„± ({need}ê°œ)...", state="running")
                        raw2 = _generate_with_openai(prompt, model) if backend == "openai" else _generate_with_gemini(prompt, model)
                        cand2 = _extract_questions(raw2, need)
                        for q in cand2:
                            if len(uniques) >= n_items: break
                            if not any(is_similar(q, pt) for pt in (prev_texts + uniques)):
                                uniques.append(q)

                    st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                    st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ê³ ê¸‰"
                    st.session_state[hist_key].extend(uniques)

                status.update(label="ì™„ë£Œ âœ…", state="complete")
        else:
            # st.status ì—†ëŠ” êµ¬ë²„ì „ í˜¸í™˜: ê°„ë‹¨ ì§„í–‰ë°”
            bar = st.progress(0)
            backend, model = _get_llm_backend()
            context = _gather_context(); bar.progress(20)
            if difficulty == "ì´ˆê¸‰":
                prompt = QUIZ_PROMPT_MC.format(category=CATEGORY_NAME, n_items=n_items, context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"))
            else:
                prompt = QUIZ_PROMPT_TXT.format(category=CATEGORY_NAME, n_items=n_items, context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"))
            raw = _generate_with_openai(prompt, model) if backend == "openai" else _generate_with_gemini(prompt, model); bar.progress(60)
            prev_texts = [p if isinstance(p, str) else p.get("q","") for p in st.session_state[hist_key]]
            if difficulty == "ì´ˆê¸‰":
                cand = _parse_mc_questions(raw, n_items)
                uniques = [it for it in cand if not any(is_similar(it["q"], pt) for pt in prev_texts)]
                st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ì´ˆê¸‰"
                st.session_state[hist_key].extend([u["q"] for u in uniques])
            else:
                cand = _extract_questions(raw, n_items)
                uniques = [q for q in cand if not any(is_similar(q, pt) for pt in prev_texts)]
                st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ê³ ê¸‰"
                st.session_state[hist_key].extend(uniques)
            bar.progress(100)
    ph.empty()

# ----- ë¬¸ì œ í‘œì‹œ + ë‹µì•ˆ ì…ë ¥ / ì±„ì  -----
items = st.session_state.get(f"{CATEGORY_NAME}_quiz_items", [])
mode  = st.session_state.get(f"{CATEGORY_NAME}_quiz_mode", "ê³ ê¸‰")

if items:
    st.markdown("### ìƒì„±ëœ ë¬¸ì œ")

    if mode == "ì´ˆê¸‰":
        # ê°ê´€ì‹ ë Œë”ë§ (LLM ì±„ì  ë¶ˆí•„ìš”, ìì²´ ì •ë‹µ ë¹„êµ)
        for i, it in enumerate(items, start=1):
            st.markdown(f"**{i}) {it['q']}**")
            key = f"{CATEGORY_NAME}_mc_{i-1}"
            choice = st.radio("ë³´ê¸° ì„ íƒ", options=it["opts"], key=key, index=None)
            st.caption("ì •ë‹µ ì„ íƒ í›„ ì•„ë˜ 'ì±„ì í•˜ê¸°'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        if st.button("ì±„ì í•˜ê¸°", type="primary", use_container_width=True):
            st.markdown("### ì±„ì  ê²°ê³¼")
            for i, it in enumerate(items, start=1):
                key = f"{CATEGORY_NAME}_mc_{i-1}"
                sel = st.session_state.get(key)
                # ì„ íƒê°’ì—ì„œ A/B/C/D ì¶”ì¶œ
                if sel:
                    sel_letter = sel.split(")")[0]
                else:
                    sel_letter = None
                correct = (sel_letter == it["answer"])
                verdict = "ì •ë‹µ" if correct else "ì˜¤ë‹µ"
                st.markdown(f"**ë¬¸í•­ #{i} ê²°ê³¼**")
                st.markdown(f"**íŒì •: {verdict}**")
                st.markdown(f"í”¼ë“œë°±: {it.get('expl','(í•´ì„¤ ì—†ìŒ)')}")
                st.markdown("---")
    else:
        # ì„œìˆ í˜• ë Œë”ë§ (ê¸°ì¡´ ìœ ì§€ + LLM ì±„ì )
        for i, qtext in enumerate(items, start=1):
            st.markdown(f"**{i}) {qtext}**")
            st.text_area(
                f"ë‹µì•ˆ ì…ë ¥ #{i}",
                key=f"{CATEGORY_NAME}_ans_{i-1}",
                height=100,
                placeholder="ì—¬ê¸°ì— ë³¸ì¸ ë‹µì•ˆì„ ì‘ì„±í•˜ì„¸ìš”."
            )

        # ì±„ì  ì¶œë ¥ í¬ë§· í™•ì • (í•­ìƒ ì¤„ë°”ê¿ˆ)
        def parse_eval(judged: str):
            s = (judged or "").strip().replace("\r\n", "\n")
            m_verdict = re.search(r"íŒì •\s*:\s*(ì •ë‹µ|ì˜¤ë‹µ)", s)
            verdict = m_verdict.group(1) if m_verdict else None
            m_feedback = re.search(r"í”¼ë“œë°±\s*:\s*(.*)", s, flags=re.S)
            feedback = m_feedback.group(1).strip() if m_feedback else None
            if not feedback and m_verdict:
                tail = s.split(m_verdict.group(0), 1)[-1].strip()
                if tail and not tail.lower().startswith("í”¼ë“œë°±"):
                    feedback = tail
            if not verdict:
                if "ì •ë‹µ" in s: verdict = "ì •ë‹µ"
                elif "ì˜¤ë‹µ" in s: verdict = "ì˜¤ë‹µ"
            if not feedback:
                feedback = ""
            return verdict, feedback

        def render_eval(judged: str):
            verdict, feedback = parse_eval(judged)
            st.markdown(f"**íŒì •: {verdict or 'íŒì • ë¶ˆëª…'}**")
            st.markdown(f"í”¼ë“œë°±: {feedback or '(ì—†ìŒ)'}")

        if st.button("ì±„ì í•˜ê¸°", type="primary", use_container_width=True):
            backend, model = _get_llm_backend()
            context = _gather_context()
            results = []
            for i, qtext in enumerate(items):
                ans = st.session_state.get(f"{CATEGORY_NAME}_ans_{i}", "").strip()
                eval_prompt = EVAL_PROMPT_TMPL.format(
                    category=CATEGORY_NAME,
                    question=qtext,
                    answer=ans if ans else "(ë¬´ì‘ë‹µ)",
                    context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
                )
                judged = _generate_with_openai(eval_prompt, model) if backend == "openai" else _generate_with_gemini(eval_prompt, model)
                results.append(judged)

            st.markdown("### ì±„ì  ê²°ê³¼")
            for i, judged in enumerate(results, start=1):
                st.markdown(f"**ë¬¸í•­ #{i} ê²°ê³¼**")
                render_eval(judged)
                st.markdown("---")
else:
    st.caption("ì•„ì§ ìƒì„±ëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤. â€˜ëœë¤ ë¬¸ì œ ìƒì„±â€™ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")