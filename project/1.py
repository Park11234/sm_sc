import os
import tempfile
from typing import List
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
import textwrap
import re

# =============================================
# File: pages/1_í¬í† ë¦¬ì†Œê·¸ë˜í”¼.py
# =============================================
import streamlit as st
from langchain.chains import RetrievalQA

st.set_page_config(page_title="í¬í† ë¦¬ì†Œê·¸ë˜í”¼", layout="wide")

st.header("1) í¬í† ë¦¬ì†Œê·¸ë˜í”¼")

# ê°œìš”
st.subheader("ê°œìš”")
st.write("ì›¨ì´í¼ í‘œë©´ì— ê°ê´‘ë§‰ì„ ë°”ë¥´ê³  ë…¸ê´‘Â·í˜„ìƒìœ¼ë¡œ íŒ¨í„´ì„ í˜•ì„±í•©ë‹ˆë‹¤.")

# í•µì‹¬ í¬ì¸íŠ¸ (íˆ´íŒ ê¸°ëŠ¥ í¬í•¨)
st.subheader("í•µì‹¬ í¬ì¸íŠ¸")
st.markdown("""
- <span title="ê°ê´‘ë§‰ì„ ì›¨ì´í¼ì— ê· ì¼í•˜ê²Œ ë„í¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">PR ì½”íŒ…</span> â†’ 
  <span title="ê°ê´‘ë§‰ì˜ ìš©ë§¤ë¥¼ ì¦ë°œì‹œì¼œ ì•ˆì •í™”ì‹œí‚¤ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">ì†Œí”„íŠ¸ë² ì´í¬</span> â†’ 
  <span title="ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ë¹›(EUV/DUV)ì„ í†µí•´ ê°ê´‘ë§‰ì— ì „ì‚¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">ë…¸ê´‘(EUV/DUV)</span> â†’ 
  <span title="ë…¸ê´‘ í›„ Bakeë¥¼ í†µí•´ í™”í•™ ë°˜ì‘ì„ ì•ˆì •í™”ì‹œí‚¤ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">PEB</span> â†’ 
  <span title="ë…¸ê´‘ëœ ì˜ì—­ì„ í˜„ìƒì•¡ìœ¼ë¡œ ì œê±°í•˜ì—¬ íŒ¨í„´ì„ í˜•ì„±í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">í˜„ìƒ</span> â†’ 
  <span title="íŒ¨í„´ì„ ê³ ì •í•˜ê³  ë‚´ì—´ì„± ë° ë‚´í™”í•™ì„±ì„ ê°•í™”í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">í•˜ë“œë² ì´í¬</span> â†’ 
  <span title="íŒ¨í„´ì˜ ê²°í•¨ ì—¬ë¶€, ì •ë ¬ ìƒíƒœ ë“±ì„ ê²€ì‚¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.">ê²€ì‚¬</span>
""", unsafe_allow_html=True)

st.markdown("- í•´ìƒë„(Î», NA, k1), í¬ì»¤ìŠ¤/ë„ì¦ˆ, LER/LWR")

st.subheader("í”„ë¡œì„¸ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨")
steps = ["Wafer Clean", "PR Coat", "Soft Bake", "Exposure", "PEB", "Develop", "Hard Bake", "Inspection"]
st.graphviz_chart("\n".join([
    "digraph G {",
    "rankdir=LR;",
    "node [shape=box, style=rounded, fontsize=12];",
    *[f"n{i} [label=\"{s}\"];" for i, s in enumerate(steps)],
    *[f"n{i} -> n{i + 1};" for i in range(len(steps) - 1)],
    "}",
]), use_container_width=True)

st.subheader("ê³µì • ë‹¨ê³„ ì„¤ëª…")
steps_data = [
    {"name": "ì›¨ì´í¼ ì„¸ì • (Wafer Clean)", "desc": "ì›¨ì´í¼ í‘œë©´ì˜ ì˜¤ì—¼ë¬¼, ì…ì ë“±ì„ ì œê±°í•˜ì—¬ ë‹¤ìŒ ê³µì •ì˜ í’ˆì§ˆì„ í™•ë³´í•©ë‹ˆë‹¤.", "icon": "ğŸ§¼"},
    {"name": "ê°ê´‘ë§‰ ë„í¬ (PR Coat)", "desc": "ê°ê´‘ë§‰ì„ ì›¨ì´í¼ì— ê· ì¼í•˜ê²Œ ë„í¬í•©ë‹ˆë‹¤.", "icon": "ğŸ§´"},
    {"name": "ì†Œí”„íŠ¸ ë² ì´í¬ (Soft Bake)", "desc": "ê°ê´‘ë§‰ì˜ ìš©ë§¤ë¥¼ ì¦ë°œì‹œì¼œ ì•ˆì •í™”ì‹œí‚¤ê³ , ë…¸ê´‘ ì‹œ íŒ¨í„´ í’ˆì§ˆì„ ë†’ì…ë‹ˆë‹¤.", "icon": "ğŸ”¥"},
    {"name": "ë…¸ê´‘ (Exposure)", "desc": "ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ë¹›(EUV/DUV)ì„ í†µí•´ ê°ê´‘ë§‰ì— ì „ì‚¬í•©ë‹ˆë‹¤.", "icon": "ğŸ’¡"},
    {"name": "PEB (Post-Exposure Bake)", "desc": "ë…¸ê´‘ í›„ Bakeë¥¼ í†µí•´ í™”í•™ ë°˜ì‘ì„ ì•ˆì •í™”ì‹œí‚¤ê³  í•´ìƒë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.", "icon": "â™¨ï¸"},
    {"name": "í˜„ìƒ (Develop)", "desc": "ë…¸ê´‘ëœ ì˜ì—­ì„ í˜„ìƒì•¡ìœ¼ë¡œ ì œê±°í•˜ì—¬ íŒ¨í„´ì„ í˜•ì„±í•©ë‹ˆë‹¤.", "icon": "ğŸ§ª"},
    {"name": "í•˜ë“œ ë² ì´í¬ (Hard Bake)", "desc": "íŒ¨í„´ì„ ê³ ì •í•˜ê³  ë‚´ì—´ì„± ë° ë‚´í™”í•™ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.", "icon": "ğŸ§±"},
    {"name": "ê²€ì‚¬ (Inspection)", "desc": "íŒ¨í„´ì˜ ê²°í•¨ ì—¬ë¶€, ì •ë ¬ ìƒíƒœ ë“±ì„ ê²€ì‚¬í•˜ì—¬ í’ˆì§ˆì„ í™•ì¸í•©ë‹ˆë‹¤.", "icon": "ğŸ”"}
]

for step in steps_data:
    with st.expander(f"{step['icon']} {step['name']}"):
        st.write(step["desc"])

st.subheader("ì§ˆì˜ì‘ë‹µ (RAG)")
if "vectorstore" not in st.session_state:
    st.info("ì„ë² ë”© ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ì—ì„œ PDF ì—…ë¡œë“œ â†’ ì„ë² ë”© ìƒì„± í›„ ì´ìš©í•˜ì„¸ìš”.")
else:
    if "qa_chain" not in st.session_state:
        backend = st.session_state.get("llm_backend", "openai")
        model = st.session_state.get("llm_model", "gpt-4o-mini")
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 4})
        if backend == "openai":
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(model=model, temperature=0.2)
        else:
            from langchain_community.chat_models import ChatOllama

            llm = ChatOllama(model=model, temperature=0.2)
        st.session_state.qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever,
                                                                return_source_documents=True)

    q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: EUVì™€ DUV ì°¨ì´")
    if st.button("ì§ˆë¬¸í•˜ê¸°", use_container_width=True):
        if q.strip():
            out = st.session_state.qa_chain({"query": q})
            st.markdown("### ë‹µë³€")
            st.write(out.get("result", "ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"))
            src = out.get("source_documents") or []
            if src:
                with st.expander("ì¶œì²˜"):
                    for i, s in enumerate(src, 1):
                        meta = s.metadata or {}
                        st.write(f"{i}. {meta.get('source', 'íŒŒì¼')} p.{meta.get('page', '?')}")
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
# ================== ëœë¤ ë¬¸ì œ ìƒì„±ê¸° + ë‹µì•ˆ ì±„ì  (ì¹´í…Œê³ ë¦¬ë³„) ==================
import re, textwrap

st.subheader("ëœë¤ ë¬¸ì œ ìƒì„±ê¸°")

# 1) ì¹´í…Œê³ ë¦¬ëª…ë§Œ í˜ì´ì§€ë³„ë¡œ ë°”ê¾¸ì„¸ìš”
CATEGORY_NAME = "í¬í† ë¦¬ì†Œê·¸ë˜í”¼"  # â† í˜ì´ì§€ ì£¼ì œì— ë§ê²Œ

# 2) ë‚œì´ë„/ë¬¸í•­ìˆ˜/ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
cols = st.columns(3)
difficulty   = cols[0].selectbox("ë‚œì´ë„", ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"], index=0)
n_items      = cols[1].selectbox("ë¬¸í•­ ìˆ˜", [1, 3, 5], index=1)
has_vs       = "vectorstore" in st.session_state
with_context = cols[2].checkbox("ì—…ë¡œë“œ ë¬¸ì„œ ê¸°ë°˜(ê¶Œì¥)", has_vs)

# 3) LLM ì¤€ë¹„ (OpenAI / Gemini, í´ë°± ê°€ëŠ¥)
def _get_llm_backend():
    return st.session_state.get("llm_backend", "openai"), st.session_state.get("llm_model", "gpt-4o")

def _generate_with_openai(prompt: str, model_name: str) -> str:
    # langchain-openai ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ OpenAI SDK í´ë°±
    try:
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model=model_name, temperature=0)
        return llm.invoke(prompt).content
    except Exception:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", ""))
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
            )
            return resp.choices[0].message.content
        except Exception as e:
            return f"__GEN_ERROR__ {e}"

def _generate_with_gemini(prompt: str, model_name: str) -> str:
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
        return llm.invoke(prompt).content
    except Exception:
        try:
            import google.generativeai as genai
            genai.configure(api_key=os.environ.get("GOOGLE_API_KEY", ""))
            model = genai.GenerativeModel(model_name)
            resp = model.generate_content(prompt)
            return resp.text
        except Exception as e:
            return f"__GEN_ERROR__ {e}"

def _gather_context(k: int = 6) -> str:
    if not has_vs or not with_context:
        return ""
    try:
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": k})
        docs = retriever.get_relevant_documents(f"{CATEGORY_NAME} í•µì‹¬ ê°œë… ìš”ì•½")
        return "\n\n".join(d.page_content for d in docs)[:6000]
    except Exception:
        return ""

def _extract_questions(s: str, expected_n: int) -> list[str]:
    "ë²ˆí˜¸/ë¶ˆë¦¿/ë¹ˆì¤„ ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ expected_nê°œ ë¬¸ì œë¥¼ ê¹”ë”íˆ ë¶„ë¦¬"
    s = s.strip()
    if not s:
        return []
    # 1) ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
    parts = re.split(r'^\s*\d+[\.\)\]]\s+', s, flags=re.M)
    parts = [p.strip() for p in parts if p.strip()]
    # ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ 2) ë¹ˆ ì¤„ ê¸°ì¤€
    if len(parts) <= 1:
        parts = [p.strip() for p in re.split(r'\n\s*\n+', s) if p.strip()]
    # ê·¸ë˜ë„ í•˜ë‚˜ë¿ì´ë©´ 3) ì¤„ ë‹¨ìœ„ë¡œ ìë¥´ê¸°(ë¬¸ì¥ì´ ê¸¸ë©´ ê·¸ëŒ€ë¡œ 1ë¬¸í•­)
    if len(parts) <= 1:
        lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
        parts = [(" ".join(lines))] if lines else []
    return parts[:expected_n]

QUIZ_PROMPT_TMPL = """\
ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê³µì • ê³¼ëª©ì˜ êµìˆ˜ì…ë‹ˆë‹¤.
ì£¼ì œ: {category}
ë‚œì´ë„: {difficulty}
ì¶œì œ ë¬¸í•­ ìˆ˜: {n_items}

{context}

ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì£¼ì œì— ë§ëŠ” ëœë¤ ì„œìˆ í˜• ë¬¸ì œë¥¼ {n_items}ê°œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ë¬¸í•­ì€ 1), 2), 3)... ì²˜ëŸ¼ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ í•œ ì¤„ì”© ì‹œì‘í•˜ì„¸ìš”.
ë‹µì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

EVAL_PROMPT_TMPL = """\
ë‹¹ì‹ ì€ {category} ë¶„ì•¼ì˜ ì±„ì  ë³´ì¡°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸í•­ê³¼ ìˆ˜í—˜ì ë‹µì•ˆì„ í‰ê°€í•˜ì„¸ìš”.

[ë¬¸í•­]
{question}

[ìˆ˜í—˜ì ë‹µì•ˆ]
{answer}

(ì„ íƒ) ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{context}

í‰ê°€ ê¸°ì¤€:
- ì‚¬ì‹¤ ì¼ì¹˜ ì—¬ë¶€, í•µì‹¬ ê°œë… í¬í•¨ ì—¬ë¶€, ë…¼ë¦¬ì„±.
- ê°„ê²°íˆ 'ì •ë‹µ' ë˜ëŠ” 'ì˜¤ë‹µ'ìœ¼ë¡œ íŒì •í•˜ê³ , 2~3ë¬¸ì¥ì˜ í”¼ë“œë°± ì œê³µ.

íŒì •ì„ í•œ í›„ í”¼ë“œë°±ì„ í•˜ì„¸ìš” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ì§€í‚¤ì„¸ìš” (ì¤„ë°”ê¿ˆ í¬í•¨, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€):
íŒì •: ì •ë‹µ|ì˜¤ë‹µ
í”¼ë“œë°±: <ë‘ì„¸ ë¬¸ì¥ í”¼ë“œë°±>
"""


# --- 4) ë¬¸ì œ ìƒì„± ---
if st.button("ëœë¤ ë¬¸ì œ ìƒì„±", use_container_width=True):
    backend, model = _get_llm_backend()
    context = _gather_context()
    prompt = QUIZ_PROMPT_TMPL.format(
        category=CATEGORY_NAME, difficulty=difficulty, n_items=n_items,
        context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
    )
    quiz_text = _generate_with_openai(prompt, model) if backend == "openai" else _generate_with_gemini(prompt, model)

    # íŒŒì‹±í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    items = _extract_questions(quiz_text, n_items)
    st.session_state[f"{CATEGORY_NAME}_quiz_items"] = items
    # ë‹µì•ˆ ì´ˆê¸°í™”
    for i in range(len(items)):
        st.session_state.pop(f"{CATEGORY_NAME}_ans_{i}", None)

# --- 5) ë¬¸ì œ í‘œì‹œ + ë‹µì•ˆ ì…ë ¥ ---
items = st.session_state.get(f"{CATEGORY_NAME}_quiz_items", [])
if items:
    st.markdown("### ìƒì„±ëœ ë¬¸ì œ")
    for i, qtext in enumerate(items, start=1):
        st.markdown(f"**{i}) {qtext}**")
        st.text_area(
            f"ë‹µì•ˆ ì…ë ¥ #{i}",
            key=f"{CATEGORY_NAME}_ans_{i-1}",
            height=100,
            placeholder="ì—¬ê¸°ì— ë³¸ì¸ ë‹µì•ˆì„ ì‘ì„±í•˜ì„¸ìš”."
        )
    def _format_eval(judged: str) -> str:
        """LLM ì¶œë ¥ì—ì„œ 'íŒì •'ê³¼ 'í”¼ë“œë°±'ì„ ë¶„ë¦¬í•´ í•­ìƒ ì¤„ë°”ê¿ˆ í˜•íƒœë¡œ ë°˜í™˜"""
        s = judged.strip().replace('\r\n', '\n')

        # 1) íŒì •/í”¼ë“œë°± ì¶”ì¶œ
        m_verdict = re.search(r'íŒì •\s*:\s*(ì •ë‹µ|ì˜¤ë‹µ)', s)
        m_feedback = re.search(r'í”¼ë“œë°±\s*:\s*(.*)', s, flags=re.S)

        if m_verdict:
            verdict = m_verdict.group(1).strip()
            feedback = (m_feedback.group(1).strip() if m_feedback else "")
            # 2) ìµœì¢… í¬ë§· (í•­ìƒ ì¤„ë°”ê¿ˆ)
            return f"íŒì •: {verdict}\ní”¼ë“œë°±: {feedback}" if feedback else f"íŒì •: {verdict}\ní”¼ë“œë°±: (ì—†ìŒ)"

        # 3) íŒì • íƒœê·¸ê°€ ì—†ìœ¼ë©´, 'í”¼ë“œë°±:' ì•ì— ê°•ì œ ì¤„ë°”ê¿ˆ ì‚½ì…
        s = re.sub(r'\s*(í”¼ë“œë°±\s*:)', r'\n\1', s).strip()
        return s

    # --- 6) ì±„ì  ---
    if st.button("ì±„ì í•˜ê¸°", type="primary", use_container_width=True):
        backend, model = _get_llm_backend()
        context = _gather_context()
        results = []
        for i, qtext in enumerate(items):
            ans = st.session_state.get(f"{CATEGORY_NAME}_ans_{i}", "").strip()
            eval_prompt = EVAL_PROMPT_TMPL.format(
                category=CATEGORY_NAME,
                question=qtext,
                answer=ans if ans else "(ë¬´ì‘ë‹µ)",
                context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
            )
            judged = _generate_with_openai(eval_prompt, model) if backend == "openai" else _generate_with_gemini(eval_prompt, model)
            results.append(judged)

        st.markdown("### ì±„ì  ê²°ê³¼")
        for i, judged in enumerate(results, start=1):
            st.markdown(f"**ë¬¸í•­ #{i} ê²°ê³¼**")
            st.markdown(_format_eval(judged))

else:
    st.caption("ì•„ì§ ìƒì„±ëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤. â€˜ëœë¤ ë¬¸ì œ ìƒì„±â€™ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
